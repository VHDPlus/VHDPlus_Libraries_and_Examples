{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### This Example creates a CNN to detect hand written numbers. The net is trained with the MNIST dataset. To optimize for FPGAs, the CNN is trained with quantizsation awareness. After training, the parameters are exported as VHDL constants."
      ],
      "metadata": {
        "id": "_9Edfh2cmCnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import everything needed"
      ],
      "metadata": {
        "id": "YQR0_U4Ek5Xq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n9KOEheBZGpN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.datasets import mnist #this library contains a lot of ML datasets including the MNIST one\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "import pickle #pickle is a library that helps us save a lot of different types of data - anything ranging from Pandas dataframes to TensorFlow models\n",
        "from tensorflow import keras\n",
        "!pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.python.core.keras import compat as tf_compat\n",
        "from tensorflow_model_optimization.python.core.quantization.keras.experimental.default_n_bit import default_n_bit_quantize_registry as n_bit_registry\n",
        "import numpy as np\n",
        "from tensorflow.python.training import moving_averages\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "quantize_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
        "quantize_model = tfmot.quantization.keras.quantize_annotate_model\n",
        "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
        "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
        "quantize_scope = tfmot.quantization.keras.quantize_scope"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the MNIST dataset"
      ],
      "metadata": {
        "id": "rNlBV656mo6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (28, 28, 1)\n",
        "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
        "\n",
        "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
        "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
        "\n",
        "train_data = train_data.astype('float32')\n",
        "test_data = test_data.astype('float32')\n",
        "\n",
        "train_data /= 255\n",
        "test_data /= 255"
      ],
      "metadata": {
        "id": "eooxZeq5aMMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52085101-8b0e-40a6-c709-348cc08cc4ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create quantization awareness for FPGA power of 2 quantization"
      ],
      "metadata": {
        "id": "o_OBiqSumuuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _FakeQuantWithMinMaxVars(inputs, min_var, max_var, per_channel, num_bits,\n",
        "                             narrow_range):\n",
        "\n",
        "  if per_channel:\n",
        "    assert len(min_var.get_shape()) == 1\n",
        "    assert len(max_var.get_shape()) == 1\n",
        "    return tf.quantization.fake_quant_with_min_max_vars_per_channel(\n",
        "        inputs, min_var, max_var, num_bits=num_bits, narrow_range=narrow_range)\n",
        "  else:\n",
        "    assert min_var.get_shape() == []  # pylint: disable=g-explicit-bool-comparison\n",
        "    assert max_var.get_shape() == []  # pylint: disable=g-explicit-bool-comparison\n",
        "    return tf.quantization.fake_quant_with_min_max_vars(\n",
        "        inputs, min_var, max_var, num_bits=num_bits, narrow_range=narrow_range)\n",
        "\n",
        "def MovingAvgQuantize(inputs,\n",
        "                      min_var,\n",
        "                      max_var,\n",
        "                      per_channel=False,\n",
        "                      ema_decay=0.999,\n",
        "                      name_prefix='MovingAvgQuantize',\n",
        "                      is_training=True,\n",
        "                      num_bits=8,\n",
        "                      narrow_range=False,\n",
        "                      symmetric=False):\n",
        "\n",
        "  with tf.name_scope(name_prefix):\n",
        "    input_shape = inputs.get_shape()\n",
        "    input_dim = len(input_shape)\n",
        "\n",
        "    if not is_training:\n",
        "      return _FakeQuantWithMinMaxVars(\n",
        "          inputs,\n",
        "          min_var,\n",
        "          max_var,\n",
        "          per_channel=per_channel,\n",
        "          num_bits=num_bits,\n",
        "          narrow_range=narrow_range)\n",
        "    if per_channel:\n",
        "      if input_dim == 2:\n",
        "        reduce_dims = [0]\n",
        "      elif input_dim == 4:\n",
        "        reduce_dims = [0, 1, 2]\n",
        "\n",
        "    if per_channel:\n",
        "      if input_dim >= 2:\n",
        "        batch_min = tf.math.reduce_min(\n",
        "            inputs, axis=reduce_dims, name='BatchMin')\n",
        "      else:\n",
        "        batch_min = inputs\n",
        "    else:\n",
        "      batch_min = tf.math.reduce_min(inputs, name='BatchMin')\n",
        "\n",
        "    # modified by sf\n",
        "    # ensure batch_min is not 0\n",
        "    batch_min = math_ops.minimum(batch_min, -0.0000001)\n",
        "    # end modified by sf\n",
        "\n",
        "    if per_channel:\n",
        "      if input_dim >= 2:\n",
        "        batch_max = tf.math.reduce_max(\n",
        "            inputs, axis=reduce_dims, name='BatchMax')\n",
        "      else:\n",
        "        batch_max = inputs\n",
        "    else:\n",
        "      batch_max = tf.math.reduce_max(inputs, name='BatchMax')\n",
        "\n",
        "    # modified by sf\n",
        "    # ensure batch_max is not 0\n",
        "    batch_max = math_ops.maximum(batch_max, 0.0000001)\n",
        "    # end modified by sf\n",
        "\n",
        "    if symmetric:\n",
        "      if narrow_range:\n",
        "        min_max_ratio = -1\n",
        "      else:\n",
        "        # In two's complement notation, the negative range is slightly larger\n",
        "        # than the positive range.\n",
        "        min_max_ratio = -((1 << num_bits) - 2) / (1 << num_bits)\n",
        "\n",
        "      # TFLite requires that 0.0 if always in the [min; max] range. Because\n",
        "      # batch_min <= batch_max, it follows that range_min <= 0 <= range_max.\n",
        "      range_min = tf.minimum(batch_min, batch_max / min_max_ratio)\n",
        "      range_max = tf.maximum(batch_max, batch_min * min_max_ratio)\n",
        "    else:\n",
        "      # TFLite requires that 0.0 if always in the [min; max] range.\n",
        "      range_min = tf.minimum(batch_min, 0.0)\n",
        "      range_max = tf.maximum(batch_max, 0.0)\n",
        "\n",
        "    assign_min = moving_averages.assign_moving_average(\n",
        "        min_var, range_min, ema_decay, zero_debias=False, name='AssignMinEma')\n",
        "    assign_max = moving_averages.assign_moving_average(\n",
        "        max_var, range_max, ema_decay, zero_debias=False, name='AssignMaxEma')\n",
        "\n",
        "    # modified by sf\n",
        "    min_log = tf.math.ceil(tf.math.log(tf.math.abs(assign_min))/tf.math.log(2.0))\n",
        "    max_log = tf.math.ceil(tf.math.log(tf.math.abs(assign_max))/tf.math.log(2.0))\n",
        "    minmax_log = math_ops.maximum(min_log,max_log)\n",
        "    min_po2 = tf.math.negative(tf.math.pow(2.0,minmax_log))\n",
        "    max_po2 = tf.math.pow(2.0,minmax_log)-tf.math.pow(2.0,minmax_log-7)\n",
        "\n",
        "    assign_min_po2 = tf_compat.assign(assign_min,min_po2,name='AssignMinPo2MovAvg')\n",
        "    assign_max_po2 = tf_compat.assign(assign_max,max_po2,name='AssignMaxPo2MovAvg')\n",
        "\n",
        "    return _FakeQuantWithMinMaxVars(\n",
        "        inputs,\n",
        "        assign_min_po2,\n",
        "        assign_max_po2,\n",
        "        per_channel=per_channel,\n",
        "        num_bits=num_bits,\n",
        "        narrow_range=narrow_range)\n",
        "\n",
        "class _QuantizeHelper(object):\n",
        "  \"\"\"Mixin with helper functions for quantizers.\"\"\"\n",
        "\n",
        "  def _add_range_weights(self, layer, name, per_axis=False, tensor_shape=None):\n",
        "    \"\"\"Add min and max vars to layer.\"\"\"\n",
        "    shape = None\n",
        "    if per_axis and tensor_shape is not None:\n",
        "      shape = (tensor_shape[-1])\n",
        "\n",
        "    min_weight = layer.add_weight(\n",
        "        name + '_min',\n",
        "        initializer=keras.initializers.Constant(-6.0),\n",
        "        trainable=False,\n",
        "        shape=shape)\n",
        "    max_weight = layer.add_weight(\n",
        "        name + '_max',\n",
        "        initializer=keras.initializers.Constant(6.0),\n",
        "        trainable=False,\n",
        "        shape=shape)\n",
        "\n",
        "    return {'min_var': min_weight, 'max_var': max_weight}\n",
        "\n",
        "class FixedRangeQuantizer(_QuantizeHelper, tfmot.quantization.keras.quantizers.Quantizer):\n",
        "  def __init__(self, num_bits = 8, per_axis = False, symmetric = True, narrow_range = True):\n",
        "    self.num_bits = num_bits\n",
        "    self.per_axis = per_axis\n",
        "    self.symmetric = symmetric\n",
        "    self.narrow_range = narrow_range\n",
        "\n",
        "  def build(self, tensor_shape, name, layer):\n",
        "    return self._add_range_weights(layer, name)\n",
        "\n",
        "  def __call__(self, inputs, training, weights, **kwargs):\n",
        "    return MovingAvgQuantize(\n",
        "        inputs,\n",
        "        weights['min_var'],\n",
        "        weights['max_var'],\n",
        "        ema_decay=0.999,\n",
        "        is_training=training,\n",
        "        num_bits=self.num_bits,\n",
        "        per_channel=self.per_axis,\n",
        "        symmetric=self.symmetric,\n",
        "        narrow_range=self.narrow_range\n",
        "    )\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\n",
        "        'num_bits': self.num_bits,\n",
        "        'per_axis': self.per_axis,\n",
        "        'symmetric': self.symmetric,\n",
        "        'narrow_range': self.narrow_range\n",
        "    }\n",
        "\n",
        "class ModifiedDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "      # Use custom algorithm defined in `FixedRangeQuantizer` instead of default Quantizer.\n",
        "      return [(layer.kernel, FixedRangeQuantizer(num_bits = 8, per_axis = False, symmetric = True, narrow_range = True))]\n",
        "      #return []\n",
        "\n",
        "    # Configure how to quantize activations.\n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "      return [(layer.activation, FixedRangeQuantizer(num_bits = 8, per_axis = False, symmetric = True, narrow_range = True))]\n",
        "      #return []\n",
        "\n",
        "    def set_quantize_weights(self, layer, quantize_weights):\n",
        "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
        "      # , in the same order\n",
        "      layer.kernel = quantize_weights[0]\n",
        "      #pass\n",
        "\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
        "      # , in the same order.\n",
        "      layer.activation = quantize_activations[0]\n",
        "      #pass\n",
        "\n",
        "    # Configure how to quantize outputs (may be equivalent to activations).\n",
        "    def get_output_quantizers(self, layer):\n",
        "      return []\n",
        "      #if self.quantize_output:\n",
        "      #  return [FixedRangeQuantizer()]\n",
        "      #return []\n",
        "\n",
        "    def get_config(self):\n",
        "      return {}\n",
        "      #return {'quantize_output': self.quantize_output}\n",
        "\n",
        "class ModifiedConvQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
        "    def get_weights_and_quantizers(self, layer):\n",
        "      # Use custom algorithm defined in `FixedRangeQuantizer` instead of default Quantizer.\n",
        "      return [(layer.kernel, FixedRangeQuantizer(num_bits = 8, per_axis = False, symmetric = True, narrow_range = True))]\n",
        "\n",
        "    # Configure how to quantize activations.\n",
        "    def get_activations_and_quantizers(self, layer):\n",
        "      return [(layer.activation, FixedRangeQuantizer(num_bits = 8, per_axis = False, symmetric = True, narrow_range = True))]\n",
        "\n",
        "    def set_quantize_weights(self, layer, quantize_weights):\n",
        "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
        "      # , in the same order\n",
        "      layer.kernel = quantize_weights[0]\n",
        "\n",
        "    def set_quantize_activations(self, layer, quantize_activations):\n",
        "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
        "      # , in the same order.\n",
        "      layer.activation = quantize_activations[0]\n",
        "\n",
        "    # Configure how to quantize outputs (may be equivalent to activations).\n",
        "    def get_output_quantizers(self, layer):\n",
        "      return []\n",
        "\n",
        "    def get_config(self):\n",
        "      return {}"
      ],
      "metadata": {
        "id": "_tQqNDqiokwK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the neural network (can be customized for different applications)"
      ],
      "metadata": {
        "id": "C002LU3lm9pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set resolution and activation max value\n",
        "weight_bits = 8\n",
        "value_bits = 9\n",
        "max_value = 2**(value_bits-(weight_bits-1))\n",
        "\n",
        "act = 'relu'\n",
        "\n",
        "model = quantize_model(keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=input_shape),\n",
        "  quantize_layer(keras.layers.Conv2D(4, (3, 3), activation=act, kernel_initializer='he_normal', padding='same', strides=(1,1)), quantize_config=ModifiedConvQuantizeConfig()),\n",
        "  keras.layers.MaxPooling2D((2, 2), padding='valid'),\n",
        "  quantize_layer(keras.layers.Conv2D(6, (3, 3), activation=act, kernel_initializer='he_normal', padding='same', strides=(1,1)), quantize_config=ModifiedConvQuantizeConfig()),\n",
        "  keras.layers.MaxPooling2D((2, 2), padding='valid'),\n",
        "  quantize_layer(keras.layers.Conv2D(8, (3, 3), activation=act, kernel_initializer='he_normal', padding='same', strides=(1,1)), quantize_config=ModifiedConvQuantizeConfig()),\n",
        "  keras.layers.MaxPooling2D((2, 2), padding='valid'),\n",
        "  quantize_layer(keras.layers.Flatten()),\n",
        "  quantize_annotate_layer(keras.layers.Dense(10, activation='softmax'), ModifiedDenseQuantizeConfig())\n",
        "]))\n",
        "\n",
        "with quantize_scope({'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}, {'ModifiedConvQuantizeConfig': ModifiedConvQuantizeConfig}):\n",
        "  # Use `quantize_apply` to actually make the model quantization aware.\n",
        "  q_aware_model = tfmot.quantization.keras.quantize_apply(model)"
      ],
      "metadata": {
        "id": "dflaWBk15l4D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the neural network"
      ],
      "metadata": {
        "id": "rQEC47mYnT78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()\n",
        "\n",
        "history = q_aware_model.fit(train_data, train_labels, epochs=10, validation_data=(test_data, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOJeMQ7_nTZC",
        "outputId": "852de98e-ff52-4b3f-9bb9-e32c6d894546"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 28, 28, 1)        3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (None, 28, 28, 4)        45        \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_max_pooling2d (Quanti  (None, 14, 14, 4)        1         \n",
            " zeWrapperV2)                                                    \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWra  (None, 14, 14, 6)        227       \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Quan  (None, 7, 7, 6)          1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_conv2d_2 (QuantizeWra  (None, 7, 7, 8)          445       \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_max_pooling2d_2 (Quan  (None, 3, 3, 8)          1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (None, 72)               1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 10)               735       \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,459\n",
            "Trainable params: 1,432\n",
            "Non-trainable params: 27\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 33s 10ms/step - loss: 0.4485 - accuracy: 0.8570 - val_loss: 0.1727 - val_accuracy: 0.9442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if everything works"
      ],
      "metadata": {
        "id": "98Inpl2inc6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_nummer = 1000\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(train_data[test_nummer], interpolation='nearest')\n",
        "plt.show()\n",
        "print(\"Number = \", train_labels[test_nummer])\n",
        "\n",
        "predict = q_aware_model.predict(train_data[test_nummer:test_nummer+1])[0]\n",
        "for o in range(0,9):\n",
        "  print('Probability for number = ' , o, ': ', np.round(predict[o]*100,5), '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "QSy5vjt03YFn",
        "outputId": "c2ce82c3-0b89-47ec-da5a-03461a653a24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number =  0\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "Probability for number =  0 :  99.65753 %\n",
            "Probability for number =  1 :  0.00124 %\n",
            "Probability for number =  2 :  0.01526 %\n",
            "Probability for number =  3 :  0.00096 %\n",
            "Probability for number =  4 :  8e-05 %\n",
            "Probability for number =  5 :  0.00719 %\n",
            "Probability for number =  6 :  0.03239 %\n",
            "Probability for number =  7 :  0.08841 %\n",
            "Probability for number =  8 :  0.18771 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract weights from trensorflow CNN"
      ],
      "metadata": {
        "id": "-dxANfgenfzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# layers: model.get_weights()\n",
        "def extract_weights(layers, log = False, quant = False):\n",
        "  # output array:\n",
        "  #  1. dimension = layer\n",
        "  #  2. dimension = filters/outputs\n",
        "  #  3. dimension = inputs\n",
        "  data_out = []\n",
        "  # parameters output:\n",
        "  #  1. dimension = layer\n",
        "  #  2. dimension:\n",
        "  #   [0] = type (1 = Conv2D, 2 = Dense, 3 = Normalization, 4 = Quantize, 5 = Middle Point)\n",
        "  #   [1] = max weight value (max weight value 1 for normalization)\n",
        "  #   [2] = input filters    (max weight value 2 for normalization)\n",
        "  #   [3] = output filters   (max weight value 3 for normalization)\n",
        "  #   [4] = rows             (max weight value 4 for normalization)\n",
        "  #   [5] = columns          (input filters for normalization)\n",
        "  #   [6] = middle point\n",
        "  #   [7] = min value(s)\n",
        "  #   [8] = max value(s)\n",
        "  #   [9] = min activation value\n",
        "  #   [10] = min activation value\n",
        "  param_out = []\n",
        "\n",
        "  # ----------- parameters for convolutional layers ------------\n",
        "  # Number of convolutional layers in this neural network\n",
        "  r_conv_layers = 0\n",
        "  # Maximum number of filters in one layer\n",
        "  r_max_filter = 0\n",
        "  # Maximum number of input values for one filter (rows * columns * input filters)\n",
        "  r_max_elements = 0\n",
        "  # Maximum values for weights and biases of every layer\n",
        "  r_max_values = []\n",
        "\n",
        "  layer_counter = 0\n",
        "  inputs = 0\n",
        "  last_was_conv = False\n",
        "  last_was_fc = False\n",
        "  last_type = 0 # 0 = No Layer, 1 = Fully Connected, 2 = Convolution\n",
        "  value_counter = 0 # 0 = bias, 1 = middle point, 2 = min values, 3 = max values, 4 = activation min, 5 = activation max\n",
        "  output_filters = 0\n",
        "  conv_matrix_size = 0\n",
        "  normalization_layer_row = 0\n",
        "\n",
        "  fc_input = 0\n",
        "  conv_row = 0\n",
        "  conv_column = 0\n",
        "\n",
        "  # go through layers in the weights array\n",
        "  for layer in layers:\n",
        "    if isinstance(layer,np.float32) or isinstance(layer,np.int32):\n",
        "      if last_type == 0:\n",
        "        if value_counter < 3:\n",
        "          if value_counter == 0:\n",
        "            # add quantize layer\n",
        "            if log:\n",
        "              print('add Quantize')\n",
        "            layer_counter = layer_counter + 1\n",
        "            data_out.append(np.zeros((3)))\n",
        "            param_out.append(np.zeros((1)))\n",
        "            param_out[layer_counter-1][0] = 4\n",
        "          data_out[layer_counter-1][value_counter] = layer\n",
        "      elif value_counter < 6:\n",
        "        if last_type == 1:\n",
        "          param_out[layer_counter-1][value_counter+5] = layer\n",
        "          #for i in range(len(data_out[layer_counter-1])):\n",
        "          #  data_out[layer_counter-1][i][inputs+value_counter] = layer\n",
        "        else:\n",
        "          param_out[layer_counter-1].append(layer)\n",
        "          #for i in range(len(data_out[layer_counter-1])):\n",
        "          #  data_out[layer_counter-1][i][inputs*conv_matrix_size+value_counter] = layer\n",
        "      else:\n",
        "        if log:\n",
        "          print('add Middle Point')\n",
        "        layer_counter = layer_counter + 1\n",
        "        data_out.append(np.zeros((1)))\n",
        "        param_out.append(np.zeros((1)))\n",
        "        data_out[layer_counter-1][0] = layer\n",
        "        param_out[layer_counter-1][0] = 5\n",
        "\n",
        "      value_counter = value_counter + 1\n",
        "    else:\n",
        "      # check if this is the bias of the last layer or the parameters of a normalization layer\n",
        "      if isinstance(layer[0],np.float32):\n",
        "        #print(\"1\")\n",
        "\n",
        "        bias_index = 0\n",
        "        if last_type == 2 and value_counter < 6 and value_counter > 0 and quant:\n",
        "          param_out[layer_counter-1].append(layer)\n",
        "        else:\n",
        "          for bias_value in layer:\n",
        "            # save the highest value for a weight or bias to calculate the bits for the integral parts of the values\n",
        "            if last_was_conv or last_was_fc:\n",
        "              if np.abs(bias_value) > param_out[layer_counter-1][1]:\n",
        "                param_out[layer_counter-1][1] = np.abs(bias_value)\n",
        "\n",
        "            # check if this is the bias of a convolutional layer\n",
        "            if last_was_conv:\n",
        "              data_out[layer_counter-1][bias_index][inputs*conv_matrix_size] = bias_value\n",
        "              normalization_layer_row = 0\n",
        "              last_type = 2\n",
        "              value_counter = 0\n",
        "            # check if this is the bias of a fully connected layer\n",
        "            elif last_was_fc:\n",
        "              data_out[layer_counter-1][bias_index][inputs] = bias_value\n",
        "              normalization_layer_row = 0\n",
        "              last_type = 1\n",
        "              value_counter = 0\n",
        "            # otherwise this must be a normaliaztion layzer\n",
        "            elif not quant:\n",
        "              if normalization_layer_row == 1 and bias_index == 0:\n",
        "                param_out.append(np.zeros(6))\n",
        "                data_out.append(np.zeros((4,len(layer))))\n",
        "                if log:\n",
        "                  print('add Normalization')\n",
        "                layer_counter = layer_counter + 1\n",
        "                # save parameters of normalization layer\n",
        "                param_out[layer_counter-1][0] = 3               # type = normalization\n",
        "                param_out[layer_counter-1][5] = len(layer)      # input values\n",
        "              if normalization_layer_row == 4:\n",
        "                bias_value = 1/np.sqrt(bias_value)\n",
        "              # check for maximum parameter for this type of parameter\n",
        "              if np.abs(bias_value) > param_out[layer_counter-1][normalization_layer_row]:\n",
        "                param_out[layer_counter-1][normalization_layer_row] = np.abs(bias_value)\n",
        "              # save parameters in list (add calculation to last parameter)\n",
        "              data_out[layer_counter-1][normalization_layer_row-1][bias_index] = bias_value\n",
        "\n",
        "            bias_index = bias_index + 1\n",
        "\n",
        "        normalization_layer_row = normalization_layer_row + 1\n",
        "        last_was_fc   = False\n",
        "        last_was_conv = False\n",
        "        value_counter = value_counter + 1\n",
        "      else:\n",
        "        for weight_row in layer:\n",
        "          # if this layer only has one row of weights, this is a fully connected layer\n",
        "          if isinstance(weight_row[0],np.float32):\n",
        "            #print(\"2\")\n",
        "\n",
        "            # check if this is the first set of weights in this fully connected layer\n",
        "            if not last_was_fc:\n",
        "              if quant:\n",
        "                param_out.append(np.zeros(11))\n",
        "              else:\n",
        "                param_out.append(np.zeros(4))\n",
        "              data_out.append(np.zeros((len(weight_row),len(layer)+1)))\n",
        "              if log:\n",
        "                print('add Dense')\n",
        "              layer_counter = layer_counter + 1\n",
        "              fc_input = 0\n",
        "              # save parameters of fully connected layer\n",
        "              param_out[layer_counter-1][0] = 2               # type = fully connected\n",
        "              param_out[layer_counter-1][2] = len(layer)      # input values\n",
        "              param_out[layer_counter-1][3] = len(weight_row) # output values\n",
        "              inputs = len(layer)\n",
        "\n",
        "            fc_output = 0\n",
        "            for weight in weight_row:\n",
        "              # save weights in list\n",
        "              data_out[layer_counter-1][fc_output][fc_input] = weight\n",
        "              fc_output = fc_output + 1\n",
        "              #\n",
        "              if np.abs(weight) > param_out[layer_counter-1][1]:\n",
        "                  param_out[layer_counter-1][1] = np.abs(weight)\n",
        "\n",
        "            fc_input = fc_input + 1\n",
        "            last_was_fc   = True\n",
        "            last_was_conv = False\n",
        "          else:\n",
        "            # go through weight columns\n",
        "            for weight_column in weight_row:\n",
        "              # a convolutional layer has for one input a list of weights for each filter\n",
        "              if not isinstance(weight_column[0],np.float32):\n",
        "                #print(\"3\")\n",
        "\n",
        "                # check if this is the first set of weights in this convolutional layer\n",
        "                if not last_was_conv:\n",
        "                  # save parameters of convolutional layer\n",
        "                  param_out.append([])\n",
        "                  data_out.append(np.zeros((len(weight_column[0]),len(layer)*len(weight_row)*len(weight_column)+1)))\n",
        "\n",
        "                  if log:\n",
        "                    print('add Conv2D')\n",
        "                  layer_counter = layer_counter + 1\n",
        "                  conv_row = 0\n",
        "                  conv_column = 0\n",
        "                  last_was_fc   = False\n",
        "                  last_was_conv = True\n",
        "                  # save parameters of fully connected layer\n",
        "                  param_out[layer_counter-1].append(1)                     # type = convolution\n",
        "                  param_out[layer_counter-1].append(0)                     # max weight = 0\n",
        "                  param_out[layer_counter-1].append(len(weight_column))    # input values\n",
        "                  param_out[layer_counter-1].append(len(weight_column[0])) # output values\n",
        "                  param_out[layer_counter-1].append(len(layer))            # rows\n",
        "                  param_out[layer_counter-1].append(len(weight_row))       # columns\n",
        "                  inputs = len(weight_column)\n",
        "                  conv_matrix_size = len(layer)*len(weight_row)\n",
        "\n",
        "                # save the highest value for a weight or bias\n",
        "                conv_input = 0\n",
        "                for input_filter in weight_column:\n",
        "                  conv_output = 0\n",
        "                  for output_filter_weight in input_filter:\n",
        "                    data_out[layer_counter-1][conv_output][conv_row*len(weight_row)*len(weight_column)+conv_column*len(weight_column)+conv_input] = output_filter_weight\n",
        "                    if np.abs(output_filter_weight) > param_out[layer_counter-1][1]:\n",
        "                      param_out[layer_counter-1][1] = np.abs(output_filter_weight)\n",
        "\n",
        "                    conv_output = conv_output + 1\n",
        "                  conv_input = conv_input + 1\n",
        "                conv_column = conv_column + 1\n",
        "            conv_row = conv_row + 1\n",
        "            conv_column = 0\n",
        "\n",
        "  return [data_out, param_out]\n",
        "\n",
        "print('Found:', len(extract_weights(q_aware_model.get_weights(), True, True)[0]), 'layers')"
      ],
      "metadata": {
        "id": "nbWHNjYKv6Oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb565c7c-f920-456e-fe5c-80bd01cc2e0a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add Quantize\n",
            "add Conv2D\n",
            "add Middle Point\n",
            "add Conv2D\n",
            "add Middle Point\n",
            "add Conv2D\n",
            "add Middle Point\n",
            "add Middle Point\n",
            "add Dense\n",
            "Found: 9 layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some helpfull functions"
      ],
      "metadata": {
        "id": "nGp4lYqdnukm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reject_outliers_2(data, m=2.):\n",
        "    d = np.abs(data - np.median(data))\n",
        "    mdev = np.median(d)\n",
        "    s = d / (mdev if mdev else 1.)\n",
        "    return data[s < m]\n",
        "\n",
        "def clip_values(int_bits, bits, negativ):\n",
        "  if int_bits < 0 and not negativ:\n",
        "    int_bits = 0\n",
        "  elif int_bits < (bits-2)*(-1):\n",
        "    int_bits = (bits-2)*(-1)\n",
        "  if int_bits > bits-2:\n",
        "    int_bits = bits-2\n",
        "  return int_bits\n",
        "\n",
        "def calc_scale(val, scale_bits):\n",
        "  if abs(val) > 0:\n",
        "    int_bits = int(np.ceil(np.log2(abs(val))))\n",
        "  else:\n",
        "    int_bits = 0\n",
        "  if int_bits > scale_bits-1:\n",
        "    int_bits = scale_bits-1"
      ],
      "metadata": {
        "id": "LsIRmlgCrVIp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export the parameters of the net for VHDP"
      ],
      "metadata": {
        "id": "Jt5BtPZWnxtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def np_to_vhdp_array(arr, param, fpga = False, bits = 8, tolerance = 0, norm = False, negativ = True, conv = 0, quant = False, scale_bits = 8, power_of_2 = False):\n",
        "  arr = np.array(arr)\n",
        "  if fpga:\n",
        "    #prüfen ob mit outlierer raus oder mit offset_min besseres ergebnis\n",
        "    #auch bei normalen weights prüfen\n",
        "    # Normalization ############################################################\n",
        "    if norm:\n",
        "      for i in range(4):\n",
        "        data = arr[i]\n",
        "\n",
        "        int_bits = int(np.ceil(np.log2(param[i+1])-tolerance))\n",
        "\n",
        "        if i == 0 or i == 3: # Wird multipliziert und 0 sollte vermieden werden\n",
        "          offset_max = int(np.ceil(np.log2(min(abs(data[data != 0])))))+(bits-5)\n",
        "          if int_bits > offset_max:\n",
        "            int_bits = int(offset_max)\n",
        "\n",
        "        int_bits = clip_values(int_bits, bits, negativ)\n",
        "        offset_type = 'G'\n",
        "        if i == 1:\n",
        "          offset_type = 'B'\n",
        "        elif i == 2:\n",
        "          offset_type = 'M'\n",
        "        elif i == 3:\n",
        "          offset_type = 'S'\n",
        "        print('  CONSTANT Normalization_',conv ,'_',offset_type,'_Offset   : INTEGER := ', int_bits, ';', sep='')\n",
        "        arr[i] = np.clip(np.round(arr[i] * 2**(bits-int_bits-1), 0), (-1)*(2**(bits-1)-1), (2**(bits-1)-1))\n",
        "      print('  CONSTANT Normalization_',conv ,' : CNN_Parameters_T(0 to 3, 0 to Normalization_',conv ,'_Values-1) :=', sep='')\n",
        "      arr = arr.astype(int)\n",
        "    else:\n",
        "      # Quantized ##############################################################\n",
        "      if quant and not power_of_2:\n",
        "        # activation scale and middle\n",
        "        scale  = (2**bits-1)/(param[10]-param[9])\n",
        "        middle = (param[10]-(param[10]-param[9])/2)*scale\n",
        "\n",
        "        int_bits = calc_scale(scale, scale_bits)\n",
        "        scale_int  = np.clip(np.round(scale  * 2**(scale_bits-int_bits-1), 0), (-1)*(2**(scale_bits-1)-1), (2**(scale_bits-1)-1)).astype(int)\n",
        "\n",
        "        if param[0] == 1:\n",
        "          print('  CONSTANT Layer_',conv ,'_Activation_Scale_Offset  : INTEGER := ', int_bits, ';', sep='')\n",
        "          print('  CONSTANT Layer_',conv ,'_Activation_Scale         : CNN_Parameter_T := ', scale_int, ';', sep='')\n",
        "        else:\n",
        "          print('  CONSTANT NN_Layer_',conv ,'_Activation_Scale_Offset  : INTEGER := ', int_bits, ';', sep='')\n",
        "          print('  CONSTANT NN_Layer_',conv ,'_Activation_Scale         : CNN_Parameter_T := ', scale_int, ';', sep='')\n",
        "\n",
        "        int_bits = calc_scale(middle, scale_bits)\n",
        "        middle_int = np.clip(np.round(middle * 2**(scale_bits-int_bits-1), 0), (-1)*(2**(scale_bits-1)-1), (2**(scale_bits-1)-1)).astype(int)\n",
        "\n",
        "        if param[0] == 1:\n",
        "          print('  CONSTANT Layer_',conv ,'_Activation_Middle_Offset : INTEGER := ', int_bits, ';', sep='')\n",
        "          print('  CONSTANT Layer_',conv ,'_Activation_Middle        : CNN_Parameter_T := ', middle_int, ';', sep='')\n",
        "        else:\n",
        "          print('  CONSTANT NN_Layer_',conv ,'_Activation_Middle_Offset : INTEGER := ', int_bits, ';', sep='')\n",
        "          print('  CONSTANT NN_Layer_',conv ,'_Activation_Middle        : CNN_Parameter_T := ', middle_int, ';', sep='')\n",
        "\n",
        "        # Comvolution ##########################################################\n",
        "        if param[0] == 1:\n",
        "          scale  = []\n",
        "          middle = []\n",
        "\n",
        "          for i in range(len(param[7])):\n",
        "            scale.append((2**bits-1)/(param[8][i]-param[7][i]))\n",
        "            middle.append((param[8][i]-(param[8][i]-param[7][i])/2)*scale[i])\n",
        "\n",
        "          max = np.amax(abs(np.asarray(scale)))\n",
        "          int_bits = calc_scale(max, scale_bits)\n",
        "          scale_int  = np.clip(np.round(np.asarray(scale)  * 2**(scale_bits-int_bits-1), 0), (-1)*(2**(scale_bits-1)-1), (2**(scale_bits-1)-1)).astype(int)\n",
        "          print('  CONSTANT Layer_',conv ,'_Scale_Offset  : INTEGER := ', int_bits, ';', sep='')\n",
        "\n",
        "          max = np.amax(abs(np.asarray(middle)))\n",
        "          int_bits = calc_scale(max, scale_bits)\n",
        "          middle_int = np.clip(np.round(np.asarray(middle) * 2**(scale_bits-int_bits-1), 0), (-1)*(2**(scale_bits-1)-1), (2**(scale_bits-1)-1)).astype(int)\n",
        "          print('  CONSTANT Layer_',conv ,'_Middle_Offset : INTEGER := ', int_bits, ';', sep='')\n",
        "\n",
        "          print('  CONSTANT Layer_',conv ,'_Scale         : CNN_Parameters_T(0 to 1, 0 to Layer_',conv ,'_Filters-1) :=', sep='')\n",
        "          print('  (')\n",
        "          print('    (', end = '')\n",
        "          start = True\n",
        "          for s in scale_int:\n",
        "            if start:\n",
        "              start = False\n",
        "            else:\n",
        "              print(', ', end = '')\n",
        "\n",
        "            print(s, end = '')\n",
        "          print('),')\n",
        "          print('    (', end = '')\n",
        "          start = True\n",
        "          for s in middle_int:\n",
        "            if start:\n",
        "              start = False\n",
        "            else:\n",
        "              print(', ', end = '')\n",
        "\n",
        "            print(s, end = '')\n",
        "          print(')')\n",
        "          print('  );')\n",
        "\n",
        "          print('  CONSTANT Layer_',conv ,' : CNN_Weights_T(0 to Layer_',conv ,'_Filters-1, 0 to Layer_',conv ,'_Inputs-1) :=', sep='')\n",
        "          for i in range(len(arr)):\n",
        "            arr[i] = np.clip(np.round(arr[i] * scale[i] + middle[i], 0), (-1)*(2**(bits-1)-1), (2**(bits-1)-1))\n",
        "          arr = arr.astype(int)\n",
        "        # Fully Connected ######################################################\n",
        "        else:\n",
        "          scale  = (2**bits-1)/(param[8]-param[7])\n",
        "          middle = (param[8]-(param[8]-param[7])/2)*scale\n",
        "\n",
        "          int_bits = calc_scale(scale, scale_bits)\n",
        "          scale_int  = np.clip(np.round(scale  * 2**(scale_bits-int_bits-1), 0), (-1)*(2**(scale_bits-1)-1), (2**(scale_bits-1)-1)).astype(int)\n",
        "          print('  CONSTANT NN_Layer_',conv ,'_Scale_Offset  : INTEGER := ', int_bits, ';', sep='')\n",
        "\n",
        "          print('  CONSTANT NN_Layer_',conv ,'_Scale         : CNN_Parameter_T := ', scale_int, ';', sep='')\n",
        "\n",
        "          int_bits = calc_scale(middle, scale_bits)\n",
        "          middle_int = np.clip(np.round(middle * 2**(scale_bits-int_bits-1), 0), (-1)*(2**(scale_bits-1)-1), (2**(scale_bits-1)-1)).astype(int)\n",
        "          print('  CONSTANT NN_Layer_',conv ,'_Middle_Offset : INTEGER := ', int_bits, ';', sep='')\n",
        "\n",
        "          print('  CONSTANT NN_Layer_',conv ,'_Middle        : CNN_Parameter_T := ', middle_int, ';', sep='')\n",
        "\n",
        "          print('  CONSTANT NN_Layer_',conv ,' : CNN_Weights_T(0 to NN_Layer_',conv ,'_Outputs-1, 0 to NN_Layer_',conv ,'_Inputs) :=', sep='')\n",
        "          arr = np.clip(np.round(arr * scale + middle, 0), (-1)*(2**(bits-1)-1), (2**(bits-1)-1)).astype(int)\n",
        "      # Float or Power of 2 Quantized ##########################################\n",
        "      else:\n",
        "        if quant:\n",
        "          # Calculate Output Offset with Activation max value\n",
        "          max = abs(param[10])\n",
        "          if abs(param[9]) > max:\n",
        "            max = abs(param[9])\n",
        "          int_bits = int(np.ceil(np.log2(max)-tolerance))\n",
        "          int_bits = clip_values(int_bits, bits, negativ)\n",
        "          if param[0] == 1:\n",
        "            print('  CONSTANT Layer_',conv ,'_Out_Offset : INTEGER := ', int_bits, ';', sep='')\n",
        "          else:\n",
        "            print('  CONSTANT NN_Layer_',conv ,'_Out_Offset : INTEGER := ', int_bits, ';', sep='')\n",
        "\n",
        "        int_bits = int(np.ceil(np.log2(param[1])-tolerance))\n",
        "        int_bits = clip_values(int_bits, bits, negativ)\n",
        "        if param[0] == 1:\n",
        "          print('  CONSTANT Layer_',conv ,'_Offset     : INTEGER := ', int_bits, ';', sep='')\n",
        "          print('  CONSTANT Layer_',conv ,' : CNN_Weights_T(0 to Layer_',conv ,'_Filters-1, 0 to Layer_',conv ,'_Inputs-1) :=', sep='')\n",
        "        else:\n",
        "          print('  CONSTANT NN_Layer_',conv ,'_Offset     : INTEGER := ', int_bits, ';', sep='')\n",
        "          print('  CONSTANT NN_Layer_',conv ,' : CNN_Weights_T(0 to NN_Layer_',conv ,'_Outputs-1, 0 to NN_Layer_',conv ,'_Inputs) :=', sep='')\n",
        "        arr = np.clip(np.round(arr * 2**(bits-int_bits-1), 0), (-1)*(2**(bits-1)-1), (2**(bits-1)-1)).astype(int)\n",
        "\n",
        "  else:\n",
        "    arr = np.round(arr,2)\n",
        "\n",
        "  print('  (')\n",
        "  first = 1\n",
        "  i = len(arr)\n",
        "  for d1 in arr:\n",
        "    i = i - 1\n",
        "    if not isinstance(d1,np.ndarray):\n",
        "      if first:\n",
        "        first = 0\n",
        "      else:\n",
        "        print(', ', end = '')\n",
        "      print(d1, end = '')\n",
        "    else:\n",
        "      print('    (', end = '')\n",
        "      first = 1\n",
        "      j = len(d1)\n",
        "      for d2 in d1:\n",
        "        j = j - 1\n",
        "        if not isinstance(d2,np.ndarray):\n",
        "          if first:\n",
        "            first = 0\n",
        "          else:\n",
        "            print(', ', end = '')\n",
        "          print(d2, end = '')\n",
        "        else:\n",
        "          print('      (', end = '')\n",
        "          first = 1\n",
        "          k = len(d2)\n",
        "          for d3 in d2:\n",
        "            k = k - 1\n",
        "            if not isinstance(d3,np.ndarray):\n",
        "              if first:\n",
        "                first = 0\n",
        "              else:\n",
        "                print(', ', end = '')\n",
        "              print(d3, end = '')\n",
        "          print(')', end = '')\n",
        "          if j:\n",
        "            print(',')\n",
        "          else:\n",
        "            print('')\n",
        "      print(')', end = '')\n",
        "      if i:\n",
        "        print(',')\n",
        "      else:\n",
        "        print('')\n",
        "  print('  );')\n",
        "  return 0\n",
        "\n",
        "fpga = True\n",
        "bits = weight_bits\n",
        "scale_bits = weight_bits\n",
        "norm_bits = weight_bits\n",
        "tolerance = 0 # 0.1\n",
        "quant = True\n",
        "power_of_2 = True\n",
        "\n",
        "if quant:\n",
        "  result = extract_weights(q_aware_model.get_weights(),False,True)\n",
        "else:\n",
        "  result = extract_weights(model.get_weights())\n",
        "\n",
        "data       = result[0]\n",
        "parameters = result[1]\n",
        "\n",
        "conv = 0\n",
        "fc = 0\n",
        "norm = 0\n",
        "for i in range(len(data)):\n",
        "  if parameters[i][0] == 1:\n",
        "    conv = conv + 1\n",
        "    print('  CONSTANT Layer_',conv ,'_Values     : NATURAL := ', int(parameters[i][2]), ';', sep='')\n",
        "    print('  CONSTANT Layer_',conv ,'_Filter_X   : NATURAL := ', int(parameters[i][5]), ';', sep='')\n",
        "    print('  CONSTANT Layer_',conv ,'_Filter_Y   : NATURAL := ', int(parameters[i][4]), ';', sep='')\n",
        "    print('  CONSTANT Layer_',conv ,'_Filters    : NATURAL := ', int(parameters[i][3]), ';', sep='')\n",
        "    print('  CONSTANT Layer_',conv ,'_Inputs     : NATURAL := ', int(parameters[i][5])*int(parameters[i][4])*int(parameters[i][2])+1, ';', sep='')\n",
        "    np_to_vhdp_array(data[i], parameters[i], fpga, bits, tolerance, False, True, conv, quant, scale_bits, power_of_2)\n",
        "    print('----------------')\n",
        "  if parameters[i][0] == 2:\n",
        "    fc = fc + 1\n",
        "    print('  CONSTANT NN_Layer_',fc ,'_Inputs     : NATURAL := ', int(parameters[i][2]), ';', sep='')\n",
        "    print('  CONSTANT NN_Layer_',fc ,'_Outputs    : NATURAL := ', int(parameters[i][3]), ';', sep='')\n",
        "    np_to_vhdp_array(data[i], parameters[i], fpga, bits, tolerance, False, True, fc, quant, scale_bits, power_of_2)\n",
        "    print('----------------')\n",
        "  if parameters[i][0] == 3:\n",
        "    norm = norm + 1\n",
        "    print('  CONSTANT Normalization_',norm ,'_Values     : NATURAL := ', int(parameters[i][5]), ';', sep='')\n",
        "    np_to_vhdp_array(data[i], parameters[i], fpga, norm_bits, tolerance, True, True, norm, False)\n",
        "    #np_to_vhdp_array(data[i], parameters[i], fpga, 9, tolerance, True, True, norm)\n",
        "    print('----------------')\n",
        "  if parameters[i][0] == 4:\n",
        "    # activation scale and middle\n",
        "    scale  = (2**bits-1)/(data[i][1]-data[i][0])\n",
        "    middle = (data[i][1]-(data[i][1]-data[i][0])/2)*scale\n",
        "\n",
        "    if abs(scale) > 0:\n",
        "      int_bits = int(np.ceil(np.log2(abs(scale))))\n",
        "    else:\n",
        "      int_bits = 0\n",
        "    if int_bits > scale_bits-1:\n",
        "      int_bits = scale_bits-1\n",
        "    scale_int  = np.clip(np.round(scale  * 2**(scale_bits-int_bits-1), 0), (-1)*(2**(scale_bits-1)-1), (2**(scale_bits-1)-1)).astype(int)\n",
        "\n",
        "    print('  CONSTANT Quantize_Scale_Offset  : INTEGER := ', int_bits, ';', sep='')\n",
        "    print('  CONSTANT Quantize_Scale         : CNN_Parameter_T := ', scale_int, ';', sep='')\n",
        "\n",
        "    if abs(middle) > 0:\n",
        "      int_bits = int(np.ceil(np.log2(abs(middle))))\n",
        "    else:\n",
        "      int_bits = 0\n",
        "    if int_bits > scale_bits-1:\n",
        "      int_bits = scale_bits-1\n",
        "    middle_int = np.clip(np.round(middle * 2**(scale_bits-int_bits-1), 0), (-1)*(2**(scale_bits-1)-1), (2**(scale_bits-1)-1)).astype(int)\n",
        "\n",
        "    print('  CONSTANT Quantize_Middle_Offset : INTEGER := ', int_bits, ';', sep='')\n",
        "    print('  CONSTANT Quantize_Middle        : CNN_Parameter_T := ', middle_int, ';', sep='')\n",
        "    print('----------------')\n",
        "  if parameters[i][0] == 5:\n",
        "    print('--Middle Point')\n",
        "    print('----------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wACNM3i0vngH",
        "outputId": "8413d23d-afe9-42f5-a785-8e2886ab105a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  CONSTANT Quantize_Scale_Offset  : INTEGER := 7;\n",
            "  CONSTANT Quantize_Scale         : CNN_Parameter_T := 127;\n",
            "  CONSTANT Quantize_Middle_Offset : INTEGER := 7;\n",
            "  CONSTANT Quantize_Middle        : CNN_Parameter_T := 127;\n",
            "----------------\n",
            "  CONSTANT Layer_1_Values     : NATURAL := 1;\n",
            "  CONSTANT Layer_1_Filter_X   : NATURAL := 3;\n",
            "  CONSTANT Layer_1_Filter_Y   : NATURAL := 3;\n",
            "  CONSTANT Layer_1_Filters    : NATURAL := 4;\n",
            "  CONSTANT Layer_1_Inputs     : NATURAL := 10;\n",
            "  CONSTANT Layer_1_Out_Offset : INTEGER := 3;\n",
            "  CONSTANT Layer_1_Offset     : INTEGER := 1;\n",
            "  CONSTANT Layer_1 : CNN_Weights_T(0 to Layer_1_Filters-1, 0 to Layer_1_Inputs-1) :=\n",
            "  (\n",
            "    (62, 59, 18, 36, 22, -24, 22, -18, 16, 0),\n",
            "    (63, -33, 1, -9, 33, 3, 39, 66, 36, -3),\n",
            "    (-1, -42, -43, 12, -32, 16, 12, 61, 26, -1),\n",
            "    (16, -24, 39, 41, -10, 11, 81, 63, 9, -3)\n",
            "  );\n",
            "----------------\n",
            "--Middle Point\n",
            "----------------\n",
            "  CONSTANT Layer_2_Values     : NATURAL := 4;\n",
            "  CONSTANT Layer_2_Filter_X   : NATURAL := 3;\n",
            "  CONSTANT Layer_2_Filter_Y   : NATURAL := 3;\n",
            "  CONSTANT Layer_2_Filters    : NATURAL := 6;\n",
            "  CONSTANT Layer_2_Inputs     : NATURAL := 37;\n",
            "  CONSTANT Layer_2_Out_Offset : INTEGER := 4;\n",
            "  CONSTANT Layer_2_Offset     : INTEGER := 0;\n",
            "  CONSTANT Layer_2 : CNN_Weights_T(0 to Layer_2_Filters-1, 0 to Layer_2_Inputs-1) :=\n",
            "  (\n",
            "    (-37, 13, -10, 43, -22, 47, 17, 33, 41, -6, 2, 1, 57, 53, 20, 11, -20, 18, 21, 55, 5, -8, 1, 67, 15, 49, -79, -27, -31, 45, -19, -10, -43, 44, -23, -55, 3),\n",
            "    (36, -8, -28, 27, -16, 3, 21, -24, 8, 66, 19, -28, -1, -5, -54, -20, -39, 5, -33, -22, 34, 29, -45, -28, -59, -35, 9, -41, -4, -28, -13, 30, 73, 13, -29, 58, 12),\n",
            "    (4, -42, 9, -1, 55, 49, 63, 9, -39, 67, 31, 14, 39, -7, -13, -29, -3, 18, -47, -23, -10, 35, 31, 19, -21, -21, 55, 2, -2, 4, 10, -20, -37, 4, 7, -13, -10),\n",
            "    (-9, 30, -31, -22, 30, 52, 16, 12, 25, 40, 86, 3, 30, -60, 15, -15, 32, 23, 60, -51, 54, 18, 38, -10, -49, 9, 31, -40, -11, -39, 11, -44, -11, 0, 62, -44, 1),\n",
            "    (-31, -64, 33, -75, -35, -21, 43, -37, -4, -41, 12, -67, 20, 24, 5, -10, 19, 46, 46, -47, 6, -14, 68, -26, 38, 23, 19, 11, 48, 0, 9, 52, 21, 27, 20, 24, 9),\n",
            "    (2, 28, 21, 27, -31, 37, 7, 38, -7, 44, 52, 50, 65, -14, -1, 34, 32, 50, 5, -12, 19, -21, 68, -10, -37, -43, 19, -54, 2, -54, 53, 9, -53, 4, 15, -19, -3)\n",
            "  );\n",
            "----------------\n",
            "--Middle Point\n",
            "----------------\n",
            "  CONSTANT Layer_3_Values     : NATURAL := 6;\n",
            "  CONSTANT Layer_3_Filter_X   : NATURAL := 3;\n",
            "  CONSTANT Layer_3_Filter_Y   : NATURAL := 3;\n",
            "  CONSTANT Layer_3_Filters    : NATURAL := 8;\n",
            "  CONSTANT Layer_3_Inputs     : NATURAL := 55;\n",
            "  CONSTANT Layer_3_Out_Offset : INTEGER := 5;\n",
            "  CONSTANT Layer_3_Offset     : INTEGER := 0;\n",
            "  CONSTANT Layer_3 : CNN_Weights_T(0 to Layer_3_Filters-1, 0 to Layer_3_Inputs-1) :=\n",
            "  (\n",
            "    (-33, -2, 12, 23, -19, 15, -56, -2, -12, 27, -27, -13, -29, 43, 21, -55, -26, -4, 4, -26, 7, -28, 6, 1, -2, -48, -3, 12, 29, 18, 27, 43, -37, 10, 31, -34, 26, 2, 22, 5, 45, 10, 1, 2, 22, -38, -5, 54, 12, 32, 17, 20, 10, -10, 10),\n",
            "    (-12, -21, 16, 14, 13, 7, 10, 0, 8, 41, 19, 46, -36, 24, 36, -16, 31, 9, 27, -3, -7, -16, 28, 13, 21, -2, 3, -25, -15, -1, -11, 27, -23, -49, -39, 19, 29, -25, -38, -19, -20, 36, 16, -17, 11, 20, 1, -6, -45, -20, 12, 45, -17, -30, -3),\n",
            "    (-12, -23, -14, -18, 25, -18, 15, -17, -40, 62, 35, -9, 40, -26, 34, 26, 28, 23, 10, -11, 45, -21, 1, 14, 25, -15, 0, 22, 5, 38, -23, 55, 35, 2, -36, -3, -29, 4, -28, 15, 10, -8, -42, -6, -9, 17, 63, -23, 45, 3, -37, -11, -52, -31, -12),\n",
            "    (26, -23, 10, 29, -54, 10, -23, 23, -24, -13, -12, -8, 12, 9, 0, -24, 19, 20, 25, -28, 27, 26, 15, 33, 13, 10, 25, -13, -8, 44, -17, -20, 3, -14, 24, 4, -21, -21, 17, -44, 46, 0, -16, -18, 7, -6, 7, -28, 11, 57, 26, -12, -3, -17, -14),\n",
            "    (-15, 4, 24, 13, 59, -23, 6, 1, 45, 29, 37, 4, -37, 18, -24, 43, 14, -3, -38, 59, 17, 9, 3, -7, 12, 31, 19, -50, 33, 10, 17, -65, -43, -7, 38, -42, 7, -38, 39, -40, -5, -26, -12, 22, -26, 41, -20, 22, 4, 63, -22, 6, -46, -50, 3),\n",
            "    (-35, -20, -29, 14, -12, 17, -22, 51, 11, -37, -9, 12, 12, 18, 11, 10, 4, 7, 10, -28, 31, 0, -32, 20, 43, 4, -7, 13, -20, 12, -15, 35, 57, -18, 9, -12, 2, 12, 12, -9, -69, 20, -12, -14, 14, 52, -21, -5, 21, 6, 28, 30, -12, 19, -2),\n",
            "    (20, 3, 25, -27, -30, -4, 37, -7, -10, 11, -11, 27, 37, -28, 35, 55, 28, 34, -1, 18, 27, -35, 5, -1, -27, -38, -5, 22, 48, -17, -51, 39, -22, -45, 43, 11, -28, 40, -18, 24, -17, 28, 43, -9, -18, -37, 3, -49, -17, -6, 16, 19, 2, -40, -9),\n",
            "    (-27, 51, -1, -47, 4, 2, 29, 27, -44, -24, -3, -25, 45, 8, 3, 2, -39, 1, 10, 55, 11, -33, 24, -23, 15, 5, -12, -20, -3, -20, -21, 30, 15, 17, -22, 20, 21, 71, -16, -24, -18, -45, 29, 21, -9, -6, -5, 22, -58, 38, -39, 19, -7, -46, 16)\n",
            "  );\n",
            "----------------\n",
            "--Middle Point\n",
            "----------------\n",
            "--Middle Point\n",
            "----------------\n",
            "  CONSTANT NN_Layer_1_Inputs     : NATURAL := 72;\n",
            "  CONSTANT NN_Layer_1_Outputs    : NATURAL := 10;\n",
            "  CONSTANT NN_Layer_1_Out_Offset : INTEGER := 5;\n",
            "  CONSTANT NN_Layer_1_Offset     : INTEGER := 0;\n",
            "  CONSTANT NN_Layer_1 : CNN_Weights_T(0 to NN_Layer_1_Outputs-1, 0 to NN_Layer_1_Inputs) :=\n",
            "  (\n",
            "    (15, -15, 27, 27, -15, -33, -21, -7, 15, -1, -28, -18, -19, -19, -16, -26, -11, -22, -28, -12, 40, -13, 6, -4, 24, 15, -8, 30, -7, 12, -30, 13, -21, 32, -10, -12, -23, -32, 14, 31, -46, -33, 3, -9, 36, -8, 9, -18, 22, -26, -29, 9, -14, 47, -35, 16, 9, -16, -13, 20, -16, -16, -5, -7, 44, -17, 42, -33, -37, 31, 8, 10, 2),\n",
            "    (-21, -10, 15, -47, -31, -40, 17, 15, 20, 31, 14, -16, 27, 33, 3, 33, -5, 40, -34, -27, 24, -43, -14, 14, 3, 29, 26, -35, -8, -10, 23, -12, -1, -31, -13, -1, 37, -10, -50, 29, 26, 39, -45, -23, 1, -38, -5, 45, 18, 9, -35, 20, 11, -39, -7, -21, 33, 3, 0, 5, 3, -9, -7, 20, 3, -36, -16, -11, 25, -26, 4, -30, 15),\n",
            "    (-12, -6, -6, -32, -5, 38, 7, 0, 13, -12, 15, -25, -2, 29, -20, -16, -32, 10, -16, -8, 12, 25, -12, -1, 15, -7, 12, 26, 29, -20, 18, -2, 11, 20, 18, -18, -38, -28, 35, -43, -7, 6, -24, -2, 2, -32, -30, 14, 15, 3, -1, -3, -1, 24, -4, -4, -12, 5, 24, 9, -14, -13, 0, 4, 32, -46, 26, 2, -20, 6, 22, -52, -1),\n",
            "    (-8, 7, -28, -20, 24, -8, 10, 21, -13, -39, 45, 34, 23, 0, -27, -81, 10, 36, -35, 11, -21, 5, -6, -9, -45, 0, 0, 27, -18, 15, 50, 16, 18, 38, 14, 2, 1, 12, -45, -36, -29, 5, -17, 24, -18, -11, 6, -20, 24, -6, -8, -12, 48, 3, -23, -36, -3, 6, 28, -13, 21, -24, 5, -47, -36, 23, 10, 4, 22, -5, -29, -20, -7),\n",
            "    (35, -8, 27, 10, -1, -28, 16, -13, 11, -36, -63, 31, -10, -26, 1, 48, -33, -24, -68, 1, 37, 3, -4, -3, 3, 20, -24, 35, 1, 7, -17, 1, -2, -39, -28, -2, -3, 19, -40, -34, 3, -12, -18, -35, -21, -19, -26, 54, -10, 1, 42, 1, -52, 10, 37, -18, 6, -12, 18, -2, -20, -4, 14, 20, -39, -9, 5, -9, 5, 7, 16, 2, 3),\n",
            "    (-34, -6, -13, 24, 1, -4, -15, -16, 25, -2, -32, -4, -31, -21, 13, -22, 14, 22, 2, 31, -18, -7, -69, -78, 2, -11, -11, -32, -28, -9, -39, 33, -10, -4, -10, 14, 10, -20, -12, 14, 29, 21, 9, 41, -36, -5, 60, -26, 4, 7, -7, -26, 14, -6, 12, -25, 33, -10, 9, 9, 38, 16, -3, -21, -20, -19, -5, 11, 25, -11, -32, -41, -3),\n",
            "    (15, -24, 6, -37, 11, -15, 3, 4, -48, 33, -32, -12, -21, -44, -37, 44, 15, 52, 17, 12, -13, -16, -33, 6, 23, 28, -25, -20, -1, -17, -21, -41, -22, -38, -36, 10, 0, -36, 21, 29, 24, 12, 4, -26, 11, -4, 15, -29, -4, 5, 21, 24, -59, -24, -38, 40, -4, -30, 7, -5, 8, 31, -3, 5, 10, 24, 36, 20, -40, 30, -12, -38, -6),\n",
            "    (-8, 14, -24, -27, 14, 11, 23, -12, 38, -46, -41, 2, 1, 15, -20, -32, -3, -33, -7, 10, -51, -55, 2, 10, -17, -7, -1, -4, 3, -13, 34, 19, 21, 6, 41, -6, -3, -10, 18, 13, -20, -12, 12, -32, -9, 17, 16, -12, -29, 33, 35, 6, -5, -32, 3, -22, 8, 5, -21, -25, -28, -28, -10, 51, -48, 21, 0, -9, 13, -27, 12, 30, 0),\n",
            "    (28, 4, 6, 24, -9, 0, 22, -24, 17, 2, -7, -20, -7, 29, -48, -17, -22, -20, 15, 12, 32, 1, -25, -14, 24, -4, 36, 15, 0, 12, -32, -5, -31, 11, 1, 10, -9, -21, -22, -29, -12, 29, 0, 25, 5, 16, 25, -18, 38, 40, 15, -45, -23, 28, -12, -30, -45, -36, -11, -28, -31, 28, -6, 0, -13, 35, -12, -5, -17, -20, 16, -41, 5),\n",
            "    (-6, 2, -22, 23, -32, 23, 10, 11, 24, 14, -15, -15, -18, 2, -39, -73, 4, -7, -4, -36, 12, -20, -6, -37, 11, 1, -16, -32, 2, 40, -48, 31, -44, -20, 21, 7, -4, -1, -24, 3, -22, 20, -28, -2, 31, -39, 0, -19, -29, -12, 27, -19, -5, -30, 8, 19, -9, 29, 2, 30, 28, -5, 29, 21, -40, -16, -8, -13, -27, -26, -1, -3, -6)\n",
            "  );\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a test image for VHDP (for simulation)"
      ],
      "metadata": {
        "id": "R4T1_V1noDES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('(');\n",
        "for r in train_data[test_nummer]:\n",
        "  i = 0\n",
        "  for c in r:\n",
        "    if i == 0:\n",
        "      print('    (', end = '')\n",
        "      i = 1\n",
        "    else:\n",
        "      print(', ', end = '')\n",
        "    if quant and not power_of_2: # Middle Point = 127\n",
        "      print(int(c*(2**(bits)-1)), end = '')\n",
        "    else:\n",
        "      print(int(c*(2**(bits-1)-1)), end = '')\n",
        "  print('),')\n",
        "print(');')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAxUG5Dc4MJG",
        "outputId": "6f797101-e764-4c4b-c476-3b4e4222e4a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 72, 126, 127, 125, 47, 2, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 48, 116, 126, 126, 115, 126, 126, 17, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 44, 69, 126, 126, 86, 33, 16, 99, 126, 94, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 53, 126, 126, 117, 25, 0, 0, 0, 5, 126, 126, 27, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 107, 126, 121, 27, 0, 0, 0, 0, 2, 106, 126, 28, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 126, 126, 65, 0, 0, 0, 0, 0, 0, 83, 126, 28, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 126, 121, 16, 0, 0, 0, 0, 0, 0, 83, 126, 28, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 63, 126, 78, 0, 0, 0, 0, 0, 0, 0, 83, 126, 28, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 9, 113, 126, 52, 0, 0, 0, 0, 0, 0, 3, 113, 126, 28, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 28, 126, 126, 43, 0, 0, 0, 0, 0, 0, 4, 126, 122, 23, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 28, 126, 126, 4, 0, 0, 0, 0, 0, 0, 4, 126, 104, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 28, 126, 126, 4, 0, 0, 0, 0, 0, 0, 52, 126, 45, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 2, 109, 126, 4, 0, 0, 0, 0, 0, 11, 114, 126, 11, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 107, 126, 4, 0, 0, 0, 0, 0, 41, 126, 125, 11, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 107, 126, 17, 0, 0, 0, 0, 10, 103, 125, 46, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 126, 59, 0, 0, 0, 1, 69, 126, 114, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 41, 126, 110, 8, 0, 0, 45, 126, 117, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 117, 126, 66, 10, 59, 118, 126, 61, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 26, 124, 126, 116, 125, 126, 85, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 57, 118, 126, 126, 66, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            "    (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n",
            ");\n"
          ]
        }
      ]
    }
  ]
}